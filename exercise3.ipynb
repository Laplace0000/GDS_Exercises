{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqtFyUJ1sSMD"
      },
      "source": [
        "# Exercise 3: Learning from Data\n",
        "\n",
        "In this exercise, you will train different types of regression and classification models on two datasets. You will be graded based on a combination of your code producing the expected results, your written responses to the questions and a passing leaderboard score in the final exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRhksUEwsSMH"
      },
      "source": [
        "### 3.1 Linear / Logistic Regression for digit recognition\n",
        "\n",
        "In this part you will experiment with two different types of prediction models, and qualitatively + quantitatively compare them. You will be working with the classic MNIST dataset, which we can load from `sklearn.datasets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nSImOGLFsSMJ"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "digits = datasets.load_digits();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn_nLW1FsSML"
      },
      "source": [
        "Here we plot the first few digits and their labels. Apparently they are in order, but our upcoming models will not rely on this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OW5cZj0LsSMM"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x300 with 12 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=12, figsize=(15, 3))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    ax.set_title(\"Label: %i\" % label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-4Ru0k2sSMN"
      },
      "source": [
        "### A: Warmup Exercise\n",
        "\n",
        "Use the `train_test_split` function from `sklearn.model_selection` twice to split the data into train, validation, and test sets, with a distribution of 80%, 10%, and 10% respectively. Use `random_state=0` for reproducibility.\n",
        "Check out the documentation to figure out how to set the parameters to get the desired split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IIqZiqKdsSMN"
      },
      "outputs": [],
      "source": [
        "# INSERT YOUR CODE HERE. ASSIGN TO THE FOLLOWING VARIABLES, X_train, X_val, X_test, y_train, y_val, y_test\n",
        "# >\n",
        "\n",
        "X_train, X_valtest, y_train, y_valtest = train_test_split(digits.images, digits.target, test_size=0.2, random_state=42, stratify=digits.target)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=42, stratify=y_valtest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNP_N373sSMO"
      },
      "source": [
        "Now let's check out the returned training data, which is of the type `numpy.ndarray`, which you might have seen already. We want to make sure the classes are balanced, e.g. we don't have a lot more 4's than 5's.\n",
        " - How many images do we have for the train, validation, and test sets?\n",
        " - What is the distribution of the training labels? Comment on whether the classes are balanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "iwhPlRS4sSMP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image amount for\n",
            "Training:  1437\n",
            "Testing:  180\n",
            "Evalutaion:  180\n",
            "\n",
            "Label counts for\n",
            "Training:  (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([142, 146, 142, 146, 145, 145, 145, 143, 139, 144])) Balance ration 1.0503597122302157\n",
            "Testing:  (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([18, 18, 17, 19, 18, 18, 18, 18, 18, 18])) Balance ration 1.1176470588235294\n",
            "Evalutaion:  (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([18, 18, 18, 18, 18, 19, 18, 18, 17, 18])) Balance ration 1.1176470588235294\n",
            "Balance ration full dataset:  1.0517241379310345\n"
          ]
        }
      ],
      "source": [
        "# CALCULATE AND WRITE YOUR ANSWERS HERE\n",
        "# >\n",
        "_, countsraw = np.unique(digits.target, return_counts=True)\n",
        "_, countstrain = np.unique(y_train, return_counts=True)\n",
        "_, countstest = np.unique(y_test, return_counts=True)\n",
        "_, countsval = np.unique(y_val, return_counts=True)\n",
        "\n",
        "print(\"Image amount for\")\n",
        "print(\"Training: \", X_train.shape[0],)\n",
        "print(\"Testing: \", X_test.shape[0])\n",
        "print(\"Evalutaion: \", X_val.shape[0])\n",
        "\n",
        "print(\"\\nLabel counts for\")\n",
        "print(\"Training: \", np.unique(y_train, return_counts=True), \"Balance ration\", max(countstrain)/min(countstrain))\n",
        "print(\"Testing: \",  np.unique(y_test, return_counts=True), \"Balance ration\", max(countstest)/min(countstest))\n",
        "print(\"Evalutaion: \",  np.unique(y_val, return_counts=True), \"Balance ration\", max(countsval)/min(countsval))\n",
        "print(\"Balance ration full dataset: \", max(countsraw)/min(countsraw))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Balance in the dataset is here determined by calculating the ratio between the label with the most and least amounts of entries.\n",
        "The full dataset is very balanced, with a less than five percent difference in size between the biggest and smallest.\n",
        "By using the classic randomstate of 42 the balance of the split sets however where awfull with the testing ratio being 2.7 and the others not farring much better.\n",
        "As the dataset is small it is difficoult to stumble into a good distibution. nvm discovered stratify, now it is weelll balanced. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndWjTdZHsSMP"
      },
      "source": [
        "### B: Programming Exercise\n",
        "\n",
        "Let's build a model that learns to predict the digit given the image. We'll first try our hand at linear regression. We will use the `LinearRegression` class from `sklearn.linear_model`. Again, check out the documentation on how to use it. We will use the `fit` method to train the model, and the `predict` method to make predictions on the test set.\n",
        "Afterwards, evaluate the model using functions from `sklearn.metrics` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im3QW3A1sSMQ"
      },
      "outputs": [],
      "source": [
        "# DEFINE A LinearRegression MODEL USING SKLEARN. ASSIGN THE MODEL TO THE VARIABLE model.\n",
        "# >\n",
        "\n",
        "# FIT THE MODEL AND MAKE PREDICTIONS ON THE TEST SET. ASSIGN THE PREDICTIONS TO THE VARIABLE y_pred.\n",
        "# >\n",
        "\n",
        "# ASSIGN YOUR MODEL'S TEST SET MSE TO THE VARIABLE mse AND ITS ACCURACY TO THE VARIABLE acc.\n",
        "# >\n",
        "\n",
        "\n",
        "# DO NOT INSERT OR CHANGE ANYTHING BELOW\n",
        "print(\"LinearRegression MSE: \", mse)\n",
        "print(\"LinearRegression accuracy: \", acc)\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
        "for ax, image, prediction in zip(axes, X_test, y_pred):\n",
        "    ax.set_axis_off()\n",
        "    image = image.reshape(8, 8)\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    ax.set_title(\"%i\" % prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTiMjFCXsSMR"
      },
      "source": [
        "### C: Programming Exercise\n",
        "\n",
        "Now, let's model the task differently.\n",
        "You will use the `LogisticRegression`, again included in the module `sklearn.linear_model`, and again, check out the documentation to see how to use it. Fit the model and make predictions on the test set.\n",
        "\n",
        "You are asked to produce output with the same structure as in the above programming exercise (two printouts and one image), but for a different model this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSIm-m3FsSMS"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mauemhttsSMS"
      },
      "source": [
        "### Written Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0j-wXktsSMS"
      },
      "source": [
        "1a) Describe the trends you observe in the results. Is there anything interesting or unexpected?\n",
        "\n",
        "1b) How do the results compare to the linear regression model?\n",
        "\n",
        "1c) What do you think is the reason for this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4LjDiJsSMS"
      },
      "source": [
        "## Exercise 3.2: Determine habitability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLeM565QsSMS"
      },
      "source": [
        "**Prediction task**: Build a logistic model for exoplanet habitability\n",
        "\n",
        "Your team is tasked with applying data science and machine learning techniques to analyze exoplanet data from the famous NASA Exoplanet Archive (NEA). Among 372 features, your goal is to build a model that predicts whether an exoplanet is potentially habitable (1) or non-habitable (0).\n",
        "\n",
        "\n",
        "**Evaluation metric**: F1-score\n",
        "\n",
        "\n",
        "Due to the highly imbalanced nature of the data—habitable planets are much rarer than their non-habitable counterparts—the evaluation metric is the F1-score, which balance precision and recall. This helps us know whether your model is truly effective at identifying the minority (habitable) class rather than simply achieving high overall accuracy by predicting the majority class.\n",
        "\n",
        "\n",
        "\n",
        "**Feature engineering**\n",
        "\n",
        "The most minimal version of a working solution should give you around 50-60% F1.\n",
        "\n",
        "You can improve this performance substantially—achieve closer to **85-90%** F1—by feature engineering and data preprocessing:\n",
        "- **Feature selection:** Only choosing relevant features (Hint: Look out for anything temperature-related)\n",
        "- **Feature creation:** Earth Similarity Index (see below)\n",
        "- **Feature transformation:** Scaling and standardization\n",
        "- **Imputation** of NaN/unknown values\n",
        "- **Oversampling** to increase the minority class\n",
        "\n",
        "Consider conducting exploratory data analysis (EDA) to gain useful insight about the dataset and leveraging a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to streamline your steps. For more information see the description on the [Kaggle page](https://www.kaggle.com/competitions/gds-exercise-3-2025-).\n",
        "\n",
        "\n",
        "Good luck, and we look forward to seeing your solutions!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp2_4LgqsSMT"
      },
      "source": [
        "### D: Programming Exercise\n",
        "\n",
        "1. Create an account on Kaggle to join the Kaggle competition: [GDS Exercise 3 – 2025](https://www.kaggle.com/competitions/gds-exercise-3-2025-). You can either use your real name, or tell us your Kaggle username, so we can grade you.\n",
        "2. Download the [training and test data](https://www.kaggle.com/competitions/gds-exercise-3-2025-/data). Check out the Dataset Description for more information about the data and your task.\n",
        "2. Load the training data file `train.npz` and split it into `train`, `val`, and `test` splits.\n",
        "3. Use a linear or logistic regression model to fit the `train` split. Use the `val` split for model selection and hyperparameter optimization, while reserving the `test` split for evaluation exclusively. (You should measure performance using f1-score)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wHn00kesSMT"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3rbN5nGHkdq"
      },
      "source": [
        "### E: Calculate the [Earth Similarity Index](https://en.wikipedia.org/wiki/Earth_Similarity_Index?wprov=sfla1) (Optional)\n",
        "Evidently, astronomers have observed Earth to be a very habitable planet (1) with their telescopes. As such, we make the remark that planets with properties similar to Earth tend to make excellent candidates for planet habitability.\n",
        "\n",
        "\n",
        "Earth Similarity Index (ESI) is a metric defined as:\n",
        "\n",
        "$$\n",
        "\\text{ESI} = \\prod_{i=1}^n \\left(1 - \\left| \\frac{x_i - x_{i,\\oplus}}{x_i + x_{i,\\oplus}} \\right| \\right)^\\frac{w_i}{n}\n",
        "$$\n",
        "\n",
        "where  \n",
        "$ x_i $ and $ x_{i,\\oplus} $ are features of the extraterrestrial body and of Earth respectively,  \n",
        "$ w_i $ is the weighted exponent of each feature, and  \n",
        "$ n $ is the total number of features.\n",
        "\n",
        "The weight assigned to each feature,\n",
        "$w_i$, are free parameters that can be chosen to emphasize certain characteristics over others, e.g. surface temperature is often given a much higher weight because maintaining liquid water is crucial.\n",
        "\n",
        "1. For each selected feature, determine $w_i$. You may default these to 1 (equal weight) or draw on domain knowledge.\n",
        "\n",
        "2. Compute the `ESI` for each exoplanet in `X_train` and `X_test`. Check whether your results approximately match those in `exampleESI.csv`\n",
        "\n",
        "3. When confident in you calculations, add `ESI` as a new column in both sets.\n",
        "\n",
        "4. Retrain your logistic model with your new `ESI` feature included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJUPXc_ElH91"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4zr0P2OsSMT"
      },
      "source": [
        "### F: Unlabelled data evaluation\n",
        "\n",
        "After downloading the unlabeled [test.npz](https://www.kaggle.com/competitions/gds-exercise-3-2025-/data), you will make predictions in `y_pred` for all of the unlabeled exoplanets in `X_test`. You can submit your best predictions to Kaggle three times/day.\n",
        "\n",
        "This programming exercise is considered **passed** if you achieve  $\\text{F1-Score} \\ge 0.80$ on the [leaderboard](https://www.kaggle.com/competitions/gds-exercise-3-2025-/leaderboard) using only a logistic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc5Qh5-csSMU"
      },
      "outputs": [],
      "source": [
        "# LOAD THE TEST SET AND MAKE PREDICTIONS. ASSIGN THE PREDICTIONS TO THE VARIABLE y_pred.\n",
        "# >\n",
        "\n",
        "sample = pd.read_csv('sample_submission.csv') # Load `sample_submission.csv` to validate original indices\n",
        "sample['P_HABITABLE'] = y_pred  # assuming y_pred contains your predictions\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "\n",
        "# NOW YOU CAN UPLOAD THE submission.csv FILE TO KAGGLE AND SEE YOUR ACCURACY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEavo41NsSMU"
      },
      "source": [
        "### Written Questions\n",
        "\n",
        "2a) Explain the different techniques and their purpose in your code for Programming Exercise D.\n",
        "\n",
        "2b) Which techniques were not useful in improving performance on this task?\n",
        "\n",
        "2c) Optional: Consider the exoplanet `Kepler-438 b` with `ESI=0.832838` and `P_HABITABLE=0`.\n",
        "Despite its high ESI, this planet gets violently nuked by its own sun every three Earth months. Discuss how a loss of granularity when designing a composite index, like the ESI, can result in masking outlier behavior.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
